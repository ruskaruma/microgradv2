# microgradv2

**microgradv2** is a modern reimagining of [Andrej Karpathy’s micrograd](https://github.com/karpathy/micrograd), built from scratch to explore the foundations of automatic differentiation and neural networks with today’s advancements.  

## Objectives
- Provide a minimal yet extensible autograd engine supporting both forward- and reverse-mode differentiation.  
- Implement core neural network components (layers, losses, optimisers) with clean, hackable code.  
- Support multiple backends (Python, NumPy, CuPy) for clarity and GPU acceleration.  
- Demonstrate modern features such as AMP, gradient checkpointing, and attention mechanisms in a lightweight framework.  

## Status
**This project is still under active development.** Expect breaking changes until the first stable release.  

## Credits
Inspired by [micrograd](https://github.com/karpathy/micrograd) by Andrej Karpathy.  
